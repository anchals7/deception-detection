{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d397743d-e24d-4a46-9732-0a90b73ff903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 0. Imports and Setup\n",
    "# -------------------------------\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1a4afd9-898a-4ca7-a6bd-d5a255dda273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf7f4007-898e-4367-8f7e-117942719925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 1. Data Loading and Frame Extraction\n",
    "# -------------------------------\n",
    "def load_annotations(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    label_map = {'truthful': 0, 'deceptive': 1}\n",
    "    df['label'] = df['class'].map(label_map)\n",
    "    return df\n",
    "\n",
    "def extract_frames(video_path, max_frames=30, resize=(224, 224)):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while len(frames) < max_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = cv2.resize(frame, resize)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa4f5bf1-d201-45f6-ab23-fb11a282bd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 2. Feature Extractor (MobileNetV2)\n",
    "# -------------------------------\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, device='cuda'):\n",
    "        self.device = device\n",
    "        mobilenet = models.mobilenet_v2(pretrained=True)\n",
    "        self.model = mobilenet.features\n",
    "        self.model.eval().to(device)\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def extract(self, video_path, max_frames=30):\n",
    "        frames = extract_frames(video_path, max_frames)\n",
    "        features = []\n",
    "        for frame in frames:\n",
    "            input_tensor = self.transform(frame).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                feat = self.model(input_tensor)\n",
    "                feat = torch.mean(feat, dim=[2, 3])\n",
    "            features.append(feat.squeeze(0).cpu().numpy())\n",
    "        return np.stack(features)\n",
    "\n",
    "def cache_features(df, video_dir, cache_dir, max_frames=30):\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    extractor = FeatureExtractor(device)\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        video_name = row['id']\n",
    "        cache_path = os.path.join(cache_dir, video_name.replace('.mp4', '.npy'))\n",
    "\n",
    "        if not os.path.exists(cache_path):\n",
    "            folder = 'Truthful' if row['label'] == 0 else 'Deceptive'\n",
    "            video_path = os.path.join(video_dir, folder, video_name)\n",
    "            try:\n",
    "                features = extractor.extract(video_path, max_frames)\n",
    "                np.save(cache_path, features)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {video_name}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c644f646-a638-478a-a336-23683e8d4c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 3. Dataset and DataLoader\n",
    "# -------------------------------\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, df, cache_dir, max_frames=30):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.cache_dir = cache_dir\n",
    "        self.max_frames = max_frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        video_name = row['id'].replace('.mp4', '.npy')\n",
    "        features_path = os.path.join(self.cache_dir, video_name)\n",
    "\n",
    "        try:\n",
    "            features = np.load(features_path)\n",
    "        except:\n",
    "            features = np.zeros((self.max_frames, 1280))\n",
    "\n",
    "        if len(features) > self.max_frames:\n",
    "            features = features[:self.max_frames]\n",
    "        elif len(features) < self.max_frames:\n",
    "            pad = np.zeros((self.max_frames - len(features), features.shape[1]))\n",
    "            features = np.vstack([features, pad])\n",
    "\n",
    "        return torch.FloatTensor(features), torch.tensor(row['label'], dtype=torch.float32)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    features, labels = zip(*batch)\n",
    "    features = torch.stack(features)\n",
    "    labels = torch.stack(labels)\n",
    "    lengths = torch.tensor([len(f) for f in features], dtype=torch.long)\n",
    "    return features, labels, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5226474e-cb7b-4bc7-a797-a6044853d294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -------------------------------\n",
    "# # 4. Model: CNN + BLSTM\n",
    "# # -------------------------------\n",
    "# class EnhancedCNNBLSTM(nn.Module):\n",
    "#     def __init__(self, input_size=1280, hidden_size=512, num_layers=3, dropout=0.5):\n",
    "#         super().__init__()\n",
    "#         self.feature_reducer = nn.Sequential(\n",
    "#             nn.Linear(input_size, 512),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "\n",
    "#         self.blstm = nn.LSTM(\n",
    "#             input_size=512,\n",
    "#             hidden_size=hidden_size,\n",
    "#             num_layers=num_layers,\n",
    "#             bidirectional=True,\n",
    "#             batch_first=True,\n",
    "#             dropout=dropout\n",
    "#         )\n",
    "#         self.lstm_norm = nn.LayerNorm(hidden_size * 2)\n",
    "\n",
    "#         self.attention = nn.Sequential(\n",
    "#             nn.Linear(hidden_size * 2, hidden_size),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Linear(hidden_size, 1, bias=False)\n",
    "#         )\n",
    "\n",
    "#         self.temporal_cnn = nn.Sequential(\n",
    "#             nn.Conv1d(hidden_size * 2, hidden_size, kernel_size=5, padding=2),\n",
    "#             nn.BatchNorm1d(hidden_size),\n",
    "#             nn.GELU(),\n",
    "#             nn.Dropout(dropout),\n",
    "#             nn.Conv1d(hidden_size, hidden_size // 2, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm1d(hidden_size // 2),\n",
    "#             nn.GELU(),\n",
    "#             nn.AdaptiveAvgPool1d(1)\n",
    "#         )\n",
    "\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(hidden_size // 2, 256),\n",
    "#             nn.GELU(),\n",
    "#             nn.Dropout(dropout),\n",
    "#             nn.LayerNorm(256),\n",
    "#             nn.Linear(256, 1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x, lengths):\n",
    "#         x = self.feature_reducer(x)\n",
    "#         packed = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "#         packed_out, _ = self.blstm(packed)\n",
    "#         out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "\n",
    "#         attn_weights = F.softmax(self.attention(out), dim=1)\n",
    "#         out = torch.sum(attn_weights * out, dim=1)\n",
    "#         out = self.lstm_norm(out)\n",
    "\n",
    "#         out = out.unsqueeze(-1)\n",
    "#         out = self.temporal_cnn(out).squeeze(-1)\n",
    "\n",
    "#         return self.classifier(out).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "64535242-9d89-41f3-b5cb-6cc6eb7ab748",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_BiLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, cnn_out_channels, cnn_kernel_size, lstm_hidden_size,\n",
    "                 lstm_num_layers, fc_hidden_size, dropout_rate, bidirectional=True):\n",
    "        super(CNN_BiLSTM, self).__init__()\n",
    "        \n",
    "        # 1D CNN expects input of shape (batch, channels=input_dim, sequence_len)\n",
    "        self.cnn = nn.Conv1d(in_channels=input_dim,\n",
    "                             out_channels=cnn_out_channels,\n",
    "                             kernel_size=cnn_kernel_size,\n",
    "                             padding=cnn_kernel_size // 2)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=cnn_out_channels,\n",
    "                            hidden_size=lstm_hidden_size,\n",
    "                            num_layers=lstm_num_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=bidirectional)\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(cnn_out_channels)\n",
    "\n",
    "        lstm_output_dim = lstm_hidden_size * (2 if bidirectional else 1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(lstm_output_dim, fc_hidden_size)\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # x: (batch_size, seq_len, input_dim)\n",
    "        x = x.permute(0, 2, 1)  # (batch_size, input_dim, seq_len)\n",
    "        x = self.bn(self.cnn(x))  # (batch_size, cnn_out_channels, seq_len)\n",
    "        x = x.permute(0, 2, 1)  # (batch_size, seq_len, cnn_out_channels)\n",
    "\n",
    "        # Optional: Adjust lengths if CNN changes time dim\n",
    "        # Skipped here because padding preserves length\n",
    "\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _ = self.lstm(packed)\n",
    "        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "\n",
    "        out, _ = torch.max(lstm_out, dim=1)  # (batch_size, lstm_output_dim)\n",
    "\n",
    "        out = self.dropout(self.fc1(out))\n",
    "        out = self.fc2(out).squeeze(1)     # (batch_size,)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a045ebf-f6a4-45b1-bc3a-42c21b405259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 5. Training & Evaluation\n",
    "# -------------------------------\n",
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for features, labels, lengths in loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features, lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for features, labels, lengths in loader:\n",
    "            features = features.to(device)\n",
    "            outputs = model(features, lengths)\n",
    "            preds = torch.sigmoid(outputs).cpu().numpy() > 0.5\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.numpy())\n",
    "    return {\n",
    "        'accuracy': accuracy_score(all_labels, all_preds),\n",
    "        'precision': precision_score(all_labels, all_preds),\n",
    "        'recall': recall_score(all_labels, all_preds),\n",
    "        'f1': f1_score(all_labels, all_preds)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "baf93810-d42c-43e9-b5e7-6d18c979505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.8, gamma=2):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "        return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "330ae6ed-78c9-40b1-abe0-991e17aafc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayush\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ayush\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:00<00:00, 21460.77it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 7926.24it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 12496.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=0.6842 | Val Acc=0.5000 | F1=0.6667\n",
      "Epoch 2: Loss=0.6582 | Val Acc=0.5833 | F1=0.7059\n",
      "Epoch 3: Loss=0.6357 | Val Acc=0.7500 | F1=0.7692\n",
      "Epoch 4: Loss=0.5955 | Val Acc=0.7500 | F1=0.7692\n",
      "Epoch 5: Loss=0.5591 | Val Acc=0.7500 | F1=0.7692\n",
      "Epoch 6: Loss=0.4946 | Val Acc=0.7500 | F1=0.7692\n",
      "Epoch 7: Loss=0.4760 | Val Acc=0.7500 | F1=0.7692\n",
      "Epoch 8: Loss=0.4267 | Val Acc=0.7500 | F1=0.7692\n",
      "Epoch 9: Loss=0.4245 | Val Acc=0.7500 | F1=0.7692\n",
      "Epoch 10: Loss=0.4028 | Val Acc=0.7500 | F1=0.7692\n",
      "Epoch 11: Loss=0.3491 | Val Acc=0.8333 | F1=0.8571\n",
      "Epoch 12: Loss=0.2887 | Val Acc=0.9167 | F1=0.9091\n",
      "Epoch 13: Loss=0.2153 | Val Acc=0.9167 | F1=0.9091\n",
      "Epoch 14: Loss=0.1624 | Val Acc=0.7500 | F1=0.7692\n",
      "Epoch 15: Loss=0.1452 | Val Acc=0.8333 | F1=0.8333\n",
      "Epoch 16: Loss=0.1474 | Val Acc=0.9167 | F1=0.9091\n",
      "Epoch 17: Loss=0.0851 | Val Acc=0.8333 | F1=0.8333\n",
      "Epoch 18: Loss=0.1499 | Val Acc=0.8333 | F1=0.8333\n",
      "Epoch 19: Loss=0.0706 | Val Acc=0.9167 | F1=0.9091\n",
      "Epoch 20: Loss=0.0540 | Val Acc=0.9167 | F1=0.9091\n",
      "Epoch 21: Loss=0.0696 | Val Acc=0.7500 | F1=0.6667\n",
      "Epoch 22: Loss=0.1818 | Val Acc=0.8333 | F1=0.8333\n",
      "Epoch 23: Loss=0.1096 | Val Acc=0.9167 | F1=0.9091\n",
      "Epoch 24: Loss=0.0575 | Val Acc=0.9167 | F1=0.9091\n",
      "Epoch 25: Loss=0.1755 | Val Acc=0.8333 | F1=0.8333\n",
      "Final Evaluation on Test Set:\n",
      "{'accuracy': 0.8, 'precision': 0.7857142857142857, 'recall': 0.8461538461538461, 'f1': 0.8148148148148148}\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 6. Full Pipeline Entry Point\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load and split dataset\n",
    "    annotations = load_annotations(\"data/annotations.csv\")\n",
    "    train_df, test_df = train_test_split(annotations, test_size=0.2, stratify=annotations['label'], random_state=42)\n",
    "    train_df, val_df = train_test_split(train_df, test_size=0.125, stratify=train_df['label'], random_state=42)\n",
    "\n",
    "    video_dir = \"data/Clips\"\n",
    "    cache_dir = \"cached_features\"\n",
    "    max_frames = 30\n",
    "\n",
    "    # Cache features if not already\n",
    "    print(\"Caching features...\")\n",
    "    cache_features(train_df, video_dir, cache_dir, max_frames)\n",
    "    cache_features(val_df, video_dir, cache_dir, max_frames)\n",
    "    cache_features(test_df, video_dir, cache_dir, max_frames)\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(VideoDataset(train_df, cache_dir, max_frames), batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(VideoDataset(val_df, cache_dir, max_frames), batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(VideoDataset(test_df, cache_dir, max_frames), batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Initialize CNN-BiLSTM model\n",
    "    model = CNN_BiLSTM(\n",
    "        input_dim=1280,\n",
    "        cnn_out_channels=128,\n",
    "        cnn_kernel_size=5,\n",
    "        lstm_hidden_size=256,\n",
    "        lstm_num_layers=2,\n",
    "        fc_hidden_size=128,\n",
    "        dropout_rate=0.5,\n",
    "        bidirectional=True\n",
    "    ).to(device)\n",
    "\n",
    "    # Optimizer with weight decay\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=5e-5,           # Reduced from 1e-4 (prevent overshooting)\n",
    "        weight_decay=1e-5\n",
    "    )\n",
    "\n",
    "    # Optional: compute class imbalance for pos_weight\n",
    "    pos_weight = torch.tensor([\n",
    "        len(train_df[train_df['label']==0]) / \n",
    "        len(train_df[train_df['label']==1])\n",
    "    ], device=device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    # criterion = FocalLoss()\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer,\n",
    "        T_0=10,            # Reset every 10 epochs\n",
    "        eta_min=1e-6        # Min learning rate\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(25):\n",
    "        loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "        scheduler.step()\n",
    "        metrics = evaluate(model, val_loader)\n",
    "        print(f\"Epoch {epoch+1}: Loss={loss:.4f} | Val Acc={metrics['accuracy']:.4f} | F1={metrics['f1']:.4f}\")\n",
    "\n",
    "    # Final test evaluation\n",
    "    print(\"Final Evaluation on Test Set:\")\n",
    "    test_metrics = evaluate(model, test_loader)\n",
    "    print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96702af3-ee8b-4ccc-a2f8-fa0928ecda9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
